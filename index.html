<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description" content="Reasoning Model is Stubborn: Diagnosing Instruction Overriding in Reasoning Models">
  <meta name="keywords" content="Reasoning, Reasoning Rigidity, Math Reasoning">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Reasoning Model is Stubborn: Diagnosing Instruction Overriding in Reasoning Models</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="icon" href="./static/images/icon.png">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
</head>

<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Reasoning Model is Stubborn:</h1>
            <h2 class="title is-2 publication-title">Diagnosing Instruction Overriding in Reasoning Models</h2>
            <div class="is-size-5">
              <span class="author-block">
                <a href="https://jadohu.github.io/" style="color:#f68946;font-weight:normal;">Doohyuk
                  Jang<sup>*</sup></a>,
              </span>
              <span class="author-block">
                <a href="https://akatigre.github.io/" style="color:#f68946;font-weight:normal;">Yoonjeon
                  Kim<sup>*</sup></a>,
              </span>
              <span class="author-block">
                <a href="https://github.com/chanjae-park?tab=repositories"
                  style="color:#f68946;font-weight:normal;">Chanjae Park</a>,
              </span>
              <span class="author-block">
                <a href="https://github.com/Hyun-Ryu" style="color:#f68946;font-weight:normal;">Hyun Ryu</a>,
              </span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=UWO1mloAAAAJ&hl=en"
                  style="color:#008AD7;font-weight:normal;">Eunho Yang</a>,
              </span>
            </div>

            <br>
            <div class="is-size-5 publication-authors">
              <span class="author-block"><b style="color:#f68946; font-weight:normal">&#x25B6 </b> KAIST; </b></span>
              <span class="author-block"><b style="color:#008AD7; font-weight:normal">&#x25B6 </b>AITRICS;</span>
              <span class="author-block">&nbsp&nbsp<sup>*</sup>Equal Contribution</span>
            </div>


            <div class="column has-text-centered">
              <div class="publication-links">
                <span class="link-block">
                  <a href="https://arxiv.org/abs/" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="https://github.com/ReasoningTrap/ReasoningTrap" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <img id="teaser" width="150%" src="images/teaser.pdf">
        <h2 class="subtitle has-text-centered">
          <p style="font-family:Times New Roman"><b>Figure 1. Reasoning Rigidity in Well-Known Math Problem and Logic
              Puzzle.
              When solving a subtly modified version of a well-known math problems (AIME) and famous logic puzzles
              (Fibonacci Rabbit and Tower of Hanoi),
              advanced reasoning models such as Qwen3-32B and OpenAI o3 default to familiar reasoning template leading
              to incorrect conclusions.</b></p>
        </h2>
      </div>
    </div>
  </section>


  <section class="section" style="background-color:#efeff081">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              Large language models have demonstrated remarkable proficiency in long and complex reasoning tasks.
              However, they frequently exhibit a problematic reliance on familiar reasoning patterns, a phenomenon we
              term <b>reasoning rigidity</b>. Despite explicit instructions from users, these models often override
              clearly stated conditions and default to habitual reasoning trajectories, leading to incorrect
              conclusions. This behavior presents significant challenges, particularly in domains such as mathematics
              and logic puzzle, where precise adherence to specified constraints is critical. To systematically
              investigate reasoning rigidity, a behavior largely unexplored in prior work, we introduce a expert-curated
              diagnostic set, <b>ReasoningTrap</b>. Our dataset includes specially modified variants of existing
              mathematical
              benchmarks, namely AIME and Math500, as well as well-known puzzles deliberately redesigned to require
              deviation from familiar reasoning strategies. Using this dataset, we identify recurring contamination
              patterns that occur when models default to ingrained reasoning. Specifically, we categorize this
              contamination into three distinctive modes: (i) Interpretation Overload, (ii) Input Distrust, and (iii)
              Partial Instruction Attention, each causing models to ignore or distort provided instructions.
            </p>
          </div>
        </div>
      </div>
      <!--/ Abstract. -->
  </section>





  <section class="section" style="background-color:#e4e4f781">
    <div class="columns is-centered has-text-centered">
      <div style="width:90%;">
        <div class="columns is-centered has-text-centered">
          <div class="column is-six-fifths">
            <!-- <h2 class="title is-3"><a href="https://aka.ms/gligen" target="_blank"><img id="painting_icon" width="3%" -->
            <!-- src="https://cdn-icons-png.flaticon.com/512/5886/5886212.png"> Demo -- Try it out </a> </h2> -->
            <h2 class="title is-3"><img id="dataset_icon" width="3%" src="images/logo.jpg"> ReasoningTrap </h2>
          </div>
        </div>
        <!-- Abstract. -->
        <div style="float:left; width:47.1%; border: 0px solid rgba(5, 130, 255, 0.534);">
          <h2 class="title is-5">I. <b>ReasoningTrap</b> Dataset Samples</h2>
          <div class="column is-five-fifths">
            <div class="columns is-centered">
              <img id="teaser" width="100%" src="images/same_box.gif">
            </div>
          </div>
        </div>
        <!--/ Abstract. -->

        <div style=" float:right; width:49.8%; border: 0px solid black;">
          <h2 class="title is-5">II. Demo Instruction</h2>
          <div class="column is-five-fifths">
            <div class="columns is-centered">
              <div class="publication-video">
                <iframe src="https://www.youtube.com/embed/-MCkU7IAGKs" title="YouTube video player" frameborder="0"
                  allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
                  allowfullscreen></iframe>
                <!--               <iframe src="https://user-images.githubusercontent.com/6631389/211468127-60dd5c69-8db9-43d2-a45d-e97b336e5249.mp4" frameborder="1" allow="autoplay; encrypted-media" allowfullscreen></iframe> -->
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-five-fifths">
          <h2 class="title is-3"><img id="painting_icon" width="5%"
              src="https://cdn-icons-png.flaticon.com/512/5379/5379860.png"> Model Designs: Efficient Training &
            Flexible Inference </h2>
        </div>
      </div>

      <div class="columns is-centered has-text-centered">
        <div class="column is-six-fifths">
          <div class="content has-text-justified">
            <ul>
              <li>GLIGEN is built upon existing pretrained diffusion models. whose original weights are <b>frozen</b> to
                preserve vast pre-trained knowledge.</li>
              <li>A new trainable Gated Self-Attention layer is added at each transformer block to absorb new grounding
                input.</li>
              <li>Each grounding token consists of two types of information: <b>semantic</b> of grounded entity (encoded
                text or image) and <b>spatial location</b> (encoded bounding box or keypoints).</li>
            </ul>
          </div>
          <img id="model" width="50%" src="images/approach_v1.png">
          <h3 class="subtitle has-text-centered">
            <p style="font-family:Times New Roman"><b>Figure 2. Gated Self-Attention is used to fuse new grounding
                tokens.</b></p>
          </h3>


        </div>
      </div>
  </section>



  <section class="section" style="background-color:#e4e4f781">
    <div class="columns is-centered has-text-centered">
      <div style="width:90%;">

        <!-- Abstract. -->
        <div style="float:left; width:47.1%; border: 0px solid black;">
          <h2 class="title is-5">I. Modulated Training</h2>
          <div class="content has-text-justified">
            Compared with other ways of using a pretrained diffusion model such as full-model finetuning, our newly
            added modulated layers are continual pre-trained on large grounding data (image-text-box) and is more
            cost-efficient. Just like Lego, one can plug and play different trained layers to enable different new
            capabilities.
          </div>

          <div class="column is-five-fifths">
            <div class="columns is-centered">
              <img id="modulated_training" width="105%" src="images/approach_change.gif">

            </div>
          </div>
        </div>
        <!--/ Abstract. -->

        <div style=" float:right; width:47.8%; border: 0px solid black;">
          <h2 class="title is-5">II. Scheduled Sampling</h2>
          <div class="content has-text-justified">
            As a favorable property of our modulated training, GLIGEN supports scheduled sampling in the diffusion
            process for inference, where the model can dynamically choose to use grounding tokens (by adding the new
            layer) or original diffusion model with good prior (by kicking out the new layer), and thus balances
            generation quality and grounding ability.
          </div>

          <div class="column is-five-fifths">
            <div class="columns is-centered">
              <img id="modulated_training" width="97%" src="images/approach_sampling.gif">

            </div>
          </div>
        </div>

      </div>
    </div>
  </section>



  <section class="section">
    <!-- Results. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-six-fifths">
        <h2 class="title is-3"><img id="painting_icon" width="3%"
            src="https://cdn-icons-png.flaticon.com/512/3515/3515174.png"> Results</h2>
      </div>
    </div>
    <!-- </div> -->
    <!--/ Results. -->
    <div class="container is-max-desktop">


      <!-- Grounedtext2img. -->
      <div class="columns is-centered">
        <div class="column is-full-width">
          <h2 class="title is-4">
            <font color="#0000ff">Text Grounded T2I Generation (Bounding box)</font>
          </h2>
          <img id="teaser" width="95%" src="images/groundtext2img.png">
          <h1>
            <p style="font-family:Times New Roman"><b>By exploiting knowledge of pretrained text2img model, GLIGEN can
                generate varieties of objects in given locations, it also supports varies of styles.</b>
          </h1>
          <br>
          <img id="teaser" width="95%" src="images/gligen_vs_dalle_images.png">
          <h1>
            <p style="font-family:Times New Roman"><b>Compared with existing text2img models such as DALLE1 and DALLE2,
                GLIGEN enables the new capability to allow grounding instruction. The text prompt and DALLE generated
                images are from <a href='https://openai.com/dall-e-2/'>OpenAI Blog</a>.</b>
          </h1>
        </div>
      </div>

      <br>
      <br>
      <br>

      <!-- counterfactual. -->
      <div class="columns is-centered">
        <div class="column is-full-width">
          <h2 class="title is-4">
            <font color="#0000ff">Spatially counterfactual generation</font>
          </h2>
          <img id="teaser" width="95%" src="images/counterfactual.png">
          <h1>
            <p style="font-family:Times New Roman"><b>By explicitly specifying object size and location, GLIGEN can
                generate spatially counterfactual results which are difficult to release through text2img model (e.g.,
                Stable Diffusion).</b>
          </h1>
        </div>
      </div>


      <br>
      <br>
      <br>

      <!-- counterfactual. -->
      <div class="columns is-centered">
        <div class="column is-full-width">
          <h2 class="title is-4">
            <font color="#0000ff">Image Grounded T2I Generation (Bounding box)</font>
          </h2>
          <img id="teaser" width="95%" src="images/image_condition.png">
          <h1>
            <p style="font-family:Times New Roman"><b>GLIGEN can also ground on reference images. Top row indicates
                reference images can provide more fine-grained details beyond text description such as style and shape
                or car. The second row shows reference image can also be used as style image in which case we find
                ground it into corner or edge of an image is sufficient.</b>
          </h1>
        </div>
      </div>


      <br>
      <br>
      <br>

      <!-- counterfactual. -->
      <!-- <div class="columns is-centered">
    <div class="column is-full-width">
      <h2 class="title is-4">Grounded T2I Generation (Keypoints)</h2>
      <img id="teaser" width="95%" src="images/keypoint.png">
      <h1>
        <p style="font-family:Times New Roman"><b>GLIGEN can also ground human keypoints while doing text-to-image generation.</b>
      </h1>                 
    </div>
  </div> -->


      <!-- <br>
  <br>
  <br> -->

      <!-- counterfactual. -->
      <div class="columns is-centered">
        <div class="column is-full-width">
          <h2 class="title is-4">
            <font color="#0000ff">Grounded Inpainting</font>
          </h2>
          <img id="teaser" width="95%" src="images/inpaint.png">
          <h1>
            <p style="font-family:Times New Roman"><b>Like other diffusion models, GLIGEN can also perform grounded
                image inpaint, which can generate objects tightly following provided bounding boxes.</b>
          </h1>
        </div>
      </div>




      <br>
      <br>
      <br>

      <div class="columns is-centered">
        <div class="column is-full-width">
          <h2 class="title is-4">
            <font color="#0000ff">Canny Map Grounded T2I Generation</font>
          </h2>
          <img id="teaser" width="98%" src="images/canny.png">
          <h1>
            <p style="font-family:Times New Roman"><b>GLIGEN results on canny maps.</b>
          </h1>
        </div>
      </div>


      <br>
      <br>
      <br>

      <div class="columns is-centered">
        <div class="column is-full-width">
          <h2 class="title is-4">
            <font color="#0000ff">Depth Map Grounded T2I Generation</font>
          </h2>
          <img id="teaser" width="98%" src="images/depth.png">
          <h1>
            <p style="font-family:Times New Roman"><b>GLIGEN results on depth maps.</b>
          </h1>
        </div>
      </div>




      <br>
      <br>
      <br>

      <div class="columns is-centered">
        <div class="column is-full-width">
          <h2 class="title is-4">
            <font color="#0000ff">Normal Map Grounded T2I Generation</font>
          </h2>
          <img id="teaser" width="98%" src="images/normal.png">
          <h1>
            <p style="font-family:Times New Roman"><b>GLIGEN results on normal maps.</b>
          </h1>
        </div>
      </div>


      <br>
      <br>
      <br>

      <div class="columns is-centered">
        <div class="column is-full-width">
          <h2 class="title is-4">
            <font color="#0000ff">Semantic Map Grounded T2I Generation</font>
          </h2>
          <img id="teaser" width="98%" src="images/sem.png">
          <h1>
            <p style="font-family:Times New Roman"><b>GLIGEN results on semantic maps.</b>
          </h1>
        </div>
      </div>




      <br>
      <br>
      <br>

      <div class="columns is-centered">
        <div class="column is-full-width">
          <h2 class="title is-4">
            <font color="#0000ff">Hed Map Grounded T2I Generation</font>
          </h2>
          <img id="teaser" width="98%" src="images/hed.png">
          <h1>
            <p style="font-family:Times New Roman"><b>GLIGEN results on hed maps.</b>
          </h1>
        </div>
      </div>

      <br>
      <br>
      <br>

      <div class="columns is-centered">
        <div class="column is-full-width">
          <h2 class="title is-4">Terms and Conditions</h2>
          <h1>
            <p style="font-family:Times New Roman">We have strict terms and conditions for using the model checkpoints
              and the demo; it is restricted to uses that follow the license agreement of <a
                href="https://github.com/CompVis/latent-diffusion">Latent Diffusion Model</a> and <a
                href="https://github.com/Stability-AI/StableDiffusion">Stable Diffusion</a>.
              </b>
          </h1>
        </div>
      </div>



      <div class="columns is-centered">
        <div class="column is-full-width">
          <h2 class="title is-4">Broader Impact</h2>
          <h1>
            <p style="font-family:Times New Roman">It is important to note that our model GLIGEN is designed for
              open-world grounded text-to-image generation with caption and various condition inputs (e.g. bounding
              box). However, we also recognize the importance of responsible AI considerations and the need to clearly
              communicate the capabilities and limitations of our research. While the grounding ability generalizes well
              to novel spatial configuration and concepts, our model may not perform well in scenarios that are out of
              scope or beyond the intended use case. We strongly discourage the misuse of our model in scenarios, where
              our technology could be used to generate misleading or malicious images. We also acknowledge the potential
              biases that may be present in the data used to train our model, and the need for ongoing evaluation and
              improvement to address these concerns. To ensure transparency and accountability, we have included a model
              card that describes the intended use cases, limitations, and potential biases of our model. We encourage
              users to refer to this model card and exercise caution when applying our technology in new contexts. We
              hope that our work will inspire further research and discussion on the ethical implications of AI and the
              importance of transparency and accountability in the development of new technologies.</a>.
              </b>
          </h1>
        </div>
      </div>



      <section class="section" id="BibTeX">
        <div class="container is-max-desktop content">
          <h2 class="title">BibTeX</h2>
          <pre><code>
@article{li2023gligen,
  author      = {Li, Yuheng and Liu, Haotian and Wu, Qingyang and Mu, Fangzhou and Yang, Jianwei and Gao, Jianfeng and Li, Chunyuan and Lee, Yong Jae},
  title       = {GLIGEN: Open-Set Grounded Text-to-Image Generation},
  publisher   = {arXiv:2301.07093},
  year        = {2023},
}
</code></pre>
        </div>
      </section>

      <section class="section" id="Acknowledgement">
        <div class="container is-max-desktop content">
          <h2 class="title">Acknowledgement</h2>
          <p>
            This website is adapted from <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a> and <a
              href="https://x-decoder-vl.github.io">X-Decoder</a>, licensed under a <a rel="license"
              href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
              Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>

            <a href='https://github.com/Computer-Vision-in-the-Wild/'><img id="painting_icon" width="10%"
                src="https://avatars.githubusercontent.com/u/97258247?s=200&v=4">
            </a>
            <b> Related Links</b> :
          <div class="content has-text-justified">
            <ul>
              <li><a href='https://github.com/Computer-Vision-in-the-Wild/'>[Computer Vision in the Wild] </a> </li>
              <li>GLIGEN: (box, concept) &#8594 image || GLIP : image &#8594 (box, concept); See grounded image
                understanding in <a href='https://github.com/microsoft/GLIP'>[GLIP]</a></li>
              <li>Modulated design and training of foundation models for image understanding <a
                  href='https://react-vl.github.io/'>[REACT]</a></li>
            </ul>

          </div>


          </p>
        </div>
      </section>


</body>

</html>